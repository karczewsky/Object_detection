{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Praca Inżynierska\n",
    "## Jakub Karczewski"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --inspect --logdir ./tensor_board --host localhost --port 9000\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from functools import reduce\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "from utilities.metrics import mean_average_precision\n",
    "from utilities.inference import infer_image\n",
    "\n",
    "DIR_INPUT = 'data/classic'\n",
    "DIR_OUTPUT = 'data/inference'\n",
    "DF_CLASSIC_TRAIN = pd.read_csv('csv_dataframes/input_classic.csv')\n",
    "DEVICE_VALIDATION = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Przygotowanie danych"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class PharmacyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, image_dir, transforms=None):\n",
    "        super().__init__()\n",
    "\n",
    "        unique_labels = dataframe['label'].unique()\n",
    "        unique_labels_dataframe = pd.DataFrame(data={\n",
    "            'label': unique_labels,\n",
    "            'label_id': [i+1 for i in range(unique_labels.shape[0])]\n",
    "        })\n",
    "\n",
    "        dataframe = dataframe.merge(unique_labels_dataframe, on='label')\n",
    "\n",
    "        self.image_names = dataframe['image'].unique()\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.unique_labels_dataframe = unique_labels_dataframe\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_name = self.image_names[index]\n",
    "        records = self.df[self.df['image'] == image_name]\n",
    "        image = cv2.imread(f'{self.image_dir}/{image_name}', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        boxes = []\n",
    "        for row in records.iterrows():\n",
    "            row = row[1]\n",
    "            boxes.append([row['xmin'], row['ymin'], row['xmax'], row['ymax']])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': torch.as_tensor(records['label_id'].values, dtype=torch.int64),\n",
    "            'image_id': torch.as_tensor([index], dtype=torch.int64),\n",
    "            'area': torch.as_tensor(area, dtype=torch.float32),\n",
    "            'iscrowd': torch.zeros((records.shape[0],), dtype=torch.uint8)\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': target['labels']\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "\n",
    "            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "\n",
    "        return image, target, image_name\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_names.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Przygotowanie modelu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Flip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.4),\n",
    "        A.ShiftScaleRotate(p=0.3),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "def get_validation_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "class FasterRcnn(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            train_dataset: torch.utils.data.dataset.Subset,\n",
    "            valid_dataset:torch.utils.data.dataset.Subset,\n",
    "            num_classes: int,\n",
    "            batch_size: int,\n",
    "            optimizer = torch.optim.AdamW,\n",
    "            optimizer_keys = None,\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "            scheduler_keys = None\n",
    "    ):\n",
    "        super(FasterRcnn, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_dataset = train_dataset\n",
    "        self.valid_dataset = valid_dataset\n",
    "        self.optimizer = optimizer\n",
    "        self.optimizer_keys = optimizer_keys\n",
    "        self.scheduler = scheduler\n",
    "        self.scheduler_keys = scheduler_keys\n",
    "\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "            pretrained=True,\n",
    "            pretrained_backbone=True,\n",
    "            trainable_backbone_layers=5\n",
    "        )\n",
    "\n",
    "        # get number of input features for the classifier\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "        # replace the pre-trained head with a new one\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "       train_loader = DataLoader(self.train_dataset,\n",
    "                                 batch_size=self.batch_size,\n",
    "                                 num_workers=6,\n",
    "                                 shuffle=True,\n",
    "                                 collate_fn=collate_fn)\n",
    "       return train_loader\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        valid_loader = DataLoader(self.valid_dataset,\n",
    "                                  batch_size=self.batch_size,\n",
    "                                  num_workers=6,\n",
    "                                  shuffle=False,\n",
    "                                  collate_fn=collate_fn)\n",
    "        return valid_loader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = [p for p in self.model.parameters() if p.requires_grad]\n",
    "\n",
    "        # optimizer = torch.optim.SGD(params, lr=self.learning_rate, momentum=0.9, weight_decay=0.0005)\n",
    "        # optimizer = torch.optim.AdamW(params, lr=self.learning_rate, weight_decay=1e-2)\n",
    "\n",
    "        optimizer = self.optimizer(params=params, **self.optimizer_keys)\n",
    "        return ({\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': self.scheduler(optimizer=optimizer, **self.scheduler_keys),\n",
    "            'interval': 'epoch',\n",
    "            'monitor': 'val_mAP'\n",
    "        })\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, targets, img_name = batch\n",
    "        targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "        # separate losses\n",
    "        loss_dict = self.model(images, targets)\n",
    "        # total loss\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # loss_dict contains training metrics:\n",
    "        # - loss_objectness: błąd informujący o rozróżnianiu obiektu od tła\n",
    "        # - loss_rpn_box_reg: błąd pozycjonowania proposal box'u przez RPN w wyniku regresji\n",
    "        # - loss_box_reg: błąd pozycjonowania anochor box'u w proposal box'ach\n",
    "        # - loss_classifier: błąd zaklasyfikowania obiektu do danej klasy\n",
    "        self.log_dict(loss_dict, prog_bar=True, logger=True)\n",
    "        self.log('main_loss', losses, prog_bar=False, logger=True)\n",
    "\n",
    "        return {'loss': losses}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, targets, img_name = batch\n",
    "        targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "        outputs = self.model(images, targets)\n",
    "\n",
    "        l_targets = []\n",
    "        l_preds = []\n",
    "\n",
    "        for pred, target in zip(outputs, targets):\n",
    "            # sometimes target['labels'] is longer than target['boxes']\n",
    "            target['labels'] = target['labels'][:len(target['boxes'])]\n",
    "\n",
    "            t_targets = torch.cat([\n",
    "                torch.full((len(target['boxes']), 1), target['image_id'].item()).to(DEVICE_VALIDATION),\n",
    "                torch.unsqueeze(target['labels'], 0).t().to(DEVICE_VALIDATION),\n",
    "                target['boxes'].to(DEVICE_VALIDATION)\n",
    "            ], dim=1)\n",
    "            l_targets.append(t_targets)\n",
    "\n",
    "\n",
    "            t_pred = torch.cat([\n",
    "                torch.full((len(pred['boxes']), 1), target['image_id'].item()).to(DEVICE_VALIDATION),\n",
    "                torch.unsqueeze(pred['labels'], 0).t().to(DEVICE_VALIDATION),\n",
    "                torch.unsqueeze(pred['scores'], 0).t().to(DEVICE_VALIDATION),\n",
    "                pred['boxes'].to(DEVICE_VALIDATION)\n",
    "            ], dim=1)\n",
    "            l_preds.append(t_pred)\n",
    "\n",
    "        t_target = reduce(lambda a, b: torch.cat([a, b], dim=0), l_targets)\n",
    "        t_pred = reduce(lambda a, b: torch.cat([a, b], dim=0), l_preds)\n",
    "\n",
    "        return {\n",
    "            'target': t_target,\n",
    "            'pred': t_pred\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        t_target = reduce(lambda a, b: torch.cat([a, b], dim=0), [x['target'] for x in val_step_outputs])\n",
    "        t_pred = reduce(lambda a, b: torch.cat([a, b], dim=0), [x['pred'] for x in val_step_outputs])\n",
    "\n",
    "        val_mean_average_precision = mean_average_precision(\n",
    "            t_pred, t_target, iou_threshold=0.5, ap_calculation='COCO'\n",
    "        )\n",
    "        self.log('val_mAP', val_mean_average_precision.item(), prog_bar=True, logger=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. (Opcjonalnie) Trening modelu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "dataset = PharmacyDataset(DF_CLASSIC_TRAIN, DIR_INPUT, transforms=get_train_transform())\n",
    "num_classes = len(DF_CLASSIC_TRAIN['label'].unique()) + 1\n",
    "\n",
    "train_cases = int(len(dataset) * 0.80)\n",
    "val_cases = len(dataset) - train_cases\n",
    "train_dataset, valid_dataset = random_split(dataset, lengths=[train_cases, val_cases], generator=torch.Generator().manual_seed(42))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jkarczewski/miniconda3/envs/torchvision/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: Checkpoint directory saved_models/ exists and is not empty. With save_top_k=1, all files in this directory will be deleted when a checkpoint is saved!\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "model = FasterRcnn(\n",
    "    train_dataset=train_dataset,\n",
    "    valid_dataset=valid_dataset,\n",
    "    num_classes=num_classes,\n",
    "    batch_size=1,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    optimizer_keys={'lr': 0.0005, 'weight_decay': 1e-2},\n",
    "    scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    scheduler_keys={'factor': 0.2, 'mode': 'min', 'patience': 10}\n",
    ")\n",
    "\n",
    "tb_logger = pl.loggers.TensorBoardLogger(\n",
    "    save_dir=\"./tensor_board\",\n",
    "    name=f'test_resnext',\n",
    "    version=f'lr=auto'\n",
    ")\n",
    "\n",
    "early_stopping = pl.callbacks.EarlyStopping(\n",
    "    patience=15,\n",
    "    mode='max',\n",
    "    monitor='val_mAP',\n",
    "    min_delta=0.0,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model_checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath='saved_models/',\n",
    "    filename=f'test_auto',\n",
    "    mode='max',\n",
    "    monitor='val_mAP',\n",
    "    save_top_k=1\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    logger=tb_logger,\n",
    "    accumulate_grad_batches=1,\n",
    "    accelerator='dp',\n",
    "    gradient_clip_val=0.5,\n",
    "    max_epochs=800,\n",
    "    auto_lr_find=True,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "# trainer.tune(model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | FasterRCNN | 41 M  \n",
      "/home/jkarczewski/miniconda3/envs/torchvision/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92de516e938e46068246852c8fb241e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ccd3e0349e764caf80e84c3d368b3878"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b69fbaa8cb24feaabfebb794d35e3c6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "568ab443d14c487aa50fea3822b723c2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb4bbf1de4e248c28104ef2f21f8481e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c37b1346ac584fa5b359593924959527"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7030e93ada0b48a0a41e22284022fe81"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. (Opcjonalnie) Załadowanie wytrenowanego modelu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "load_pretrained = True\n",
    "\n",
    "if load_pretrained:\n",
    "    # model with .ckpt extension\n",
    "    model = FasterRcnn.load_from_checkpoint('example_model.ckpt', df=DF_TRAIN, learning_rate=0.007, batch_size=1)\n",
    "model.freeze()\n",
    "model.eval()\n",
    "model.to('cuda')\n",
    "\n",
    "print('Model ready for inference')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Test modelu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = PharmacyDataset(DF_TRAIN, DIR_INPUT, transforms=get_validation_transform())\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    num_workers=6,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "for (images, targets, image_names) in iter(data_loader):\n",
    "    images = list(image.to('cuda') for image in images)\n",
    "    targets = [{k: v.to('cuda') for k, v in t.items()} for t in targets]\n",
    "\n",
    "    for (image, target, image_name) in zip(images, targets, image_names):\n",
    "        # image -> tensor of shape(3, 1200, 1600)\n",
    "        # target -> dict with fields: boxes(tensor of shape(N, 4)), image_id(tensor of shape(1)),...\n",
    "        boxes = target['boxes'].cpu().numpy().astype(np.int32)\n",
    "        sample = image.permute(1,2,0).cpu().numpy()\n",
    "\n",
    "        infer_image(\n",
    "            model=model,\n",
    "            image=image,\n",
    "            show_image=False,\n",
    "            labels_dict=dataset.unique_labels_dataframe,\n",
    "            score_threshold=0.5,\n",
    "            save_dir=DIR_OUTPUT,\n",
    "            save_name=image_name\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Próba wyuczenia modelu na bazie 'faceów' produktów"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PharmacyDatasetFaces(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, image_dir, transforms=None):\n",
    "        super().__init__()\n",
    "\n",
    "        unique_labels = dataframe['label'].unique()\n",
    "        unique_labels_dataframe = pd.DataFrame(data={\n",
    "            'label': unique_labels,\n",
    "            'label_id': [i+1 for i in range(unique_labels.shape[0])]\n",
    "        })\n",
    "\n",
    "        dataframe = dataframe.merge(unique_labels_dataframe, on='label')\n",
    "\n",
    "        self.image_names = dataframe['image'].unique()\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.unique_labels_dataframe = unique_labels_dataframe\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_name = self.image_names[index]\n",
    "        records = self.df[self.df['image'] == image_name]\n",
    "\n",
    "        image = cv2.imread(f'{self.image_dir}/{image_name}', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        boxes = []\n",
    "        for row in records.iterrows():\n",
    "            row = row[1]\n",
    "            boxes.append([row['xmin'], row['ymin'], row['xmax'], row['ymax']])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': torch.as_tensor(records['label_id'].values, dtype=torch.int64),\n",
    "            'image_id': torch.as_tensor([index], dtype=torch.int64),\n",
    "            'area': torch.as_tensor(area, dtype=torch.float32),\n",
    "            'iscrowd': torch.zeros((records.shape[0],), dtype=torch.uint8)\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': target['labels']\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "\n",
    "            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "\n",
    "        return image, target, image_name\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_names.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}